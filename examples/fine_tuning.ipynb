{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c75a16",
   "metadata": {},
   "source": [
    "# Fine-tune the pretrained CHGNet for better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# install CHGNet (only needed on Google Colab or if you didn't install CHGNet yet)\n",
    "!git clone --depth 1 https://github.com/CederGroupHub/chgnet\n",
    "!pip install ./chgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHGNet initialized with 400,438 parameters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "\n",
    "from chgnet.model import CHGNet\n",
    "\n",
    "chgnet = CHGNet.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16eeae1e",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208fa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from chgnet import ROOT\n",
    "\n",
    "    lmo = Structure.from_file(f\"{ROOT}/examples/o-LiMnO2_unit.cif\")\n",
    "except Exception:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    url = \"https://github.com/CederGroupHub/chgnet/raw/main/examples/o-LiMnO2_unit.cif\"\n",
    "    cif = urlopen(url).read().decode(\"utf-8\")\n",
    "    lmo = Structure.from_str(cif, fmt=\"cif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ec2524a",
   "metadata": {},
   "source": [
    "We create a dummy fine-tuning dataset by using CHGNet prediction with some random noise. For your purpose on fine-tuning to specific chemical system or AIMD data, please modify the block below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structures, energies_per_atom, forces, stresses, magmoms = [], [], [], [], []\n",
    "\n",
    "for _i in range(100):\n",
    "    structure = lmo.copy()\n",
    "    # stretch the cell by a small amount\n",
    "    structure.apply_strain(np.random.uniform(-0.1, 0.1, size=(3)))\n",
    "    # perturb all atom positions by a small amount\n",
    "    structure.perturb(0.1)\n",
    "\n",
    "    pred = chgnet.predict_structure(structure)\n",
    "\n",
    "    structures.append(structure)\n",
    "    energies_per_atom.append(pred[\"e\"] + np.random.uniform(-0.1, 0.1, size=1))\n",
    "    forces.append(pred[\"f\"] + np.random.uniform(-0.01, 0.01, size=pred[\"f\"].shape))\n",
    "    stresses.append(\n",
    "        pred[\"s\"] * -10 + np.random.uniform(-0.05, 0.05, size=pred[\"s\"].shape)\n",
    "    )\n",
    "    magmoms.append(pred[\"m\"] + np.random.uniform(-0.03, 0.03, size=pred[\"m\"].shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "052536e6",
   "metadata": {},
   "source": [
    "Note that the magmom output from CHGNet is in unit of GPa, here the -10 unit conversion modifies it to be kbar in VASP raw unit. We do this since by default, StructureData dataset class takes in VASP units.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aea8393e",
   "metadata": {},
   "source": [
    "## 2. Define DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chgnet.data.dataset import StructureData, get_train_val_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StructureData(\n",
    "    structures=structures,\n",
    "    energies=energies_per_atom,\n",
    "    forces=forces,\n",
    "    stresses=stresses,  # can be None\n",
    "    magmoms=magmoms,  # can be None\n",
    ")\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset, batch_size=8, train_ratio=0.9, val_ratio=0.05\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1dc5ad3",
   "metadata": {},
   "source": [
    "Here the batch_size is defined to be 8 for small gpu-memory. If > 10 GB memory is available, we highly recommend increase the batch_size for better speed.\n",
    "\n",
    "If you have so many structures (which is highly typical from AIMD), it's ineffecient to put them all at once into the python list as it's probably impossible for memory issue. In this case we highly recommend you to pre-convert all the structures into graphs and save them using examples/make_graphs.py. And later you can directly train CHGNet by loading the graphs from hard-drive instead of memory using the GraphData class defined in data/dataset.py\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99445e44",
   "metadata": {},
   "source": [
    "## 3. Define model and trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chgnet.trainer import Trainer\n",
    "\n",
    "# Load pretrained CHGNet\n",
    "chgnet = CHGNet.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3afbc75",
   "metadata": {},
   "source": [
    "It's optional to freeze the weights inside some layers. This is a common technique to retain the learned knowledge during fine-tuning in large pretrained neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally fix the weights of some layers\n",
    "for layer in [\n",
    "    chgnet.atom_embedding,\n",
    "    chgnet.bond_embedding,\n",
    "    chgnet.angle_embedding,\n",
    "    chgnet.bond_basis_expansion,\n",
    "    chgnet.angle_basis_expansion,\n",
    "    chgnet.atom_conv_layers,\n",
    "    chgnet.bond_conv_layers,\n",
    "    chgnet.angle_layers,\n",
    "]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52511a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=chgnet,\n",
    "    targets=\"efsm\",\n",
    "    optimizer=\"Adam\",\n",
    "    scheduler=\"CosLR\",\n",
    "    criterion=\"MSE\",\n",
    "    epochs=5,\n",
    "    learning_rate=0,\n",
    "    use_device=\"cpu\",\n",
    "    print_freq=6,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f47fca4",
   "metadata": {},
   "source": [
    "## 4. Start training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a990258",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "169de30e",
   "metadata": {},
   "source": [
    "After training, the trained model can be found in the directory of today's date. Or it can be accessed by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa383b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "best_model = trainer.best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abcc09b4",
   "metadata": {},
   "source": [
    "## Extras 1: GGA / GGA+U compatibility\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a977284",
   "metadata": {},
   "source": [
    "### Q: Why and when do you care about this?\n",
    "\n",
    "**When**: If you want to fine-tune the pretrained CHGNet with your own GGA+U VASP calculations, and you want to keep your VASP energy compatible to the pretrained dataset. In case your dataset is so large that the pretrained knowledge does not matter to you, you can ignore this.\n",
    "\n",
    "**Why**: CHGNet is trained on both GGA and GGA+U calculations from Materials Project. And there has been developed methods in solving the compatibility between GGA and GGA+U calculations which makes the energies universally applicable for cross-chemistry comparison and phase-diagram constructions. Please refer to:\n",
    "\n",
    "https://journals.aps.org/prb/abstract/10.1103/PhysRevB.84.045115\n",
    "\n",
    "Below we show an example to apply the compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagine this is the VASP raw energy\n",
    "chgnet = CHGNet.load()\n",
    "VASP_raw_energy = chgnet.predict_structure(lmo)[\"e\"] * len(lmo)\n",
    "print(f\"The raw total energy from VASP of LMO is: {VASP_raw_energy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cce51fa9",
   "metadata": {},
   "source": [
    "You can look for the energy correction applied to each element in :\n",
    "\n",
    "https://github.com/materialsproject/pymatgen/blob/v2023.2.28/pymatgen/entries/MP2020Compatibility.yaml\n",
    "\n",
    "Here LiMnO2 applies to both Mn in transition metal oxides correction and oxide correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_Mn = lmo.composition.as_dict()[\"Mn3+\"]\n",
    "Mn_correction_in_TMO = -1.668\n",
    "num_O = lmo.composition.as_dict()[\"O2-\"]\n",
    "Oxide_correction = -0.687\n",
    "\n",
    "corrected_energy = (\n",
    "    VASP_raw_energy + num_Mn * Mn_correction_in_TMO + num_O * Oxide_correction\n",
    ")\n",
    "print(\n",
    "    f\"The corrected total energy of LMO after MP2020Compatibility = {corrected_energy}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deeb3814",
   "metadata": {},
   "source": [
    "Now use this corrected energy as labels to tune CHGNet, you're good to go!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c36fe61",
   "metadata": {},
   "source": [
    "## Extras 2: AtomRef\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbb93bc0",
   "metadata": {},
   "source": [
    "If you want to fine tune CHGNet to DFT labels that are even more incompatible with Materials Project, like r2SCAN functional, or other DFTs like Gaussian or QE. More trick has to be done to withhold the most amount of information learned during pretraining.\n",
    "\n",
    "For example, formation energy can be a well-compatible property across different functionals. In CHGNet, we use a Atom_Ref operation, which is a formation-energy-like calculation for per-element contribution to the total energy.\n",
    "\n",
    "When fine-tuning to other functionals that might have large discrepancy in elemental energies. We recommend you to refit the AtomRef. So that the finetuning on the graph layers can be focused on energy contribution from atom-atom interaction instead of meaningless atom reference energies.\n",
    "\n",
    "Below I will show an example to refit the AtomRef layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The pretrained Atom_Ref (per atom reference energy):\")\n",
    "for param in chgnet.composition_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of structures / graphs\n",
    "structures = [\n",
    "    lmo,\n",
    "    Structure(\n",
    "        species=[\"Li\", \"Mn\", \"Mn\", \"O\", \"O\", \"O\"],\n",
    "        lattice=np.random.rand(3, 3),\n",
    "        coords=np.random.rand(6, 3),\n",
    "    ),\n",
    "    Structure(\n",
    "        species=[\"Li\", \"Li\", \"Mn\", \"O\", \"O\", \"O\"],\n",
    "        lattice=np.random.rand(3, 3),\n",
    "        coords=np.random.rand(6, 3),\n",
    "    ),\n",
    "    Structure(\n",
    "        species=[\"Li\", \"Mn\", \"Mn\", \"O\", \"O\", \"O\", \"O\"],\n",
    "        lattice=np.random.rand(3, 3),\n",
    "        coords=np.random.rand(7, 3),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# A list of energy_per_atom values (random values here)\n",
    "energies_per_atom = [5.5, 6, 4.8, 5.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa551c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chgnet.model.composition_model import AtomRef\n",
    "\n",
    "print(\"We initialize another identical AtomRef layers\")\n",
    "new_AtomRef = AtomRef(is_intensive=True)\n",
    "new_AtomRef.initialize_from_MPtrj()\n",
    "for param in new_AtomRef.parameters():\n",
    "    print(param[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28726cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_AtomRef.fit(structures, energies_per_atom)\n",
    "print(\"After refitting, the AtomRef looks like:\")\n",
    "for i in new_AtomRef.parameters():\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
